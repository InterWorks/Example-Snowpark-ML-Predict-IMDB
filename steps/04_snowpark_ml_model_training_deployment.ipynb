{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ML Modeling\n",
    "\n",
    "In this notebook, we will illustrate how to train an XGBoost model with the scooby doo dataset using the Snowpark ML Model API. <br>\n",
    "We also show how to do inference and deploy the model as a UDF.\n",
    "\n",
    "The Snowpark ML Model API currently supports sklearn, xgboost, and lightgbm models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.version import VERSION\n",
    "from snowflake.snowpark.functions import udf\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "# Snowpark ML\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "\n",
    "# data science libs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# misc\n",
    "import json\n",
    "import joblib\n",
    "import cachetools\n",
    "\n",
    "# warning suppresion\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Establish secure connection to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('creds.json') as f:\n",
    "    connection_parameters = json.load(f)\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "snowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('\\nConnection Established with the following parameters:')\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the table name where we stored the scooby_doo dataset\n",
    "DEMO_TABLE = 'scooby_clean'\n",
    "input_tbl = f\"{session.get_current_database()}.{session.get_current_schema()}.{DEMO_TABLE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we read in the data from a Snowflake table into a Snowpark DataFrame\n",
    "scooby_df = session.table(input_tbl)\n",
    "\n",
    "# Filter out rows where IMDB is NULL\n",
    "scooby_df = scooby_df.filter(F.not_(F.is_null(F.col(\"IMDB\"))))\n",
    "print(scooby_df.count())\n",
    "scooby_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize all the features for modeling\n",
    "\n",
    "CATEGORICAL_COLUMNS = [\"FORMAT\",\"NETWORK\",\"SETTING_TERRAIN\",\"MOTIVE\",\"MONSTER_GENDER\",\"CULPRIT_GENDER\"]\n",
    "CATEGORICAL_COLUMNS_OE = [\"FORMAT_OHE\",\"NETWORK_OHE\",\"SETTING_TERRAIN_OHE\",\"MOTIVE_OHE\",\"MONSTER_GENDER_OHE\",\"CULPRIT_GENDER_OHE\"]\n",
    "\n",
    "NUMERICAL_COLUMNS = [\"ENGAGEMENT\",\"RUN_TIME\",\"ZOINKS\",\"GROOVY\",\"SCOOBY_DOO_WHERE_ARE_YOU\",\"ROOBY_ROOBY_ROO\"]\n",
    "NUMERICAL_COLUMNS_NORM = [\"ENGAGEMENT_NORM\",\"RUN_TIME_NORM\",\"ZOINKS_NORM\",\"GROOVY_NORM\",\"SCOOBY_DOO_WHERE_ARE_YOU_NORM\",\"ROOBY_ROOBY_ROO_NORM\"]\n",
    "\n",
    "LABEL_COLUMNS = ['IMDB']\n",
    "OUTPUT_COLUMNS = ['IMDB_PRICE']\n",
    "\n",
    "scooby_ml_df = scooby_df.select(LABEL_COLUMNS + CATEGORICAL_COLUMNS + NUMERICAL_COLUMNS)\n",
    "scooby_ml_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(scooby_ml_df.select(F.col(\"CULPRIT_GENDER\")).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scooby_df = session.read.options({\"field_delimiter\": \",\", \n",
    "                                    \"skip_header\": 1,\n",
    "                                    \"field_optionally_enclosed_by\": '\"'}).schema(scooby_schema).csv(\"@SCOOBY_ASSETS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessing pipeline object\n",
    "PIPELINE_FILE = 'preprocessing_pipeline.joblib'\n",
    "preprocessing_pipeline = joblib.load(PIPELINE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build a simple XGBoost Regression model\n",
    "\n",
    "\n",
    "- The `model.fit()` function creates a temporary stored procedure in the background. This means that the model training is a single-node operation. If more memory is needed, Snowflake has the Snowpark Optimised Wharehouse which has 16x more memory than a standard warehouse. For now we are just using a standard x-small warehouse created in the setup step.\n",
    "- The `model.predict()` function creates a temporary vectorized UDF in the background, which means the input DataFrame is batched as Pandas DataFrames and inference is parallelized across the batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "scooby_train_df, scooby_test_df = scooby_ml_df.random_split(weights=[0.9, 0.1], seed=0)\n",
    "\n",
    "# Run the train and test sets through the Pipeline object we defined earlier\n",
    "train_df = preprocessing_pipeline.fit(scooby_train_df).transform(scooby_train_df)\n",
    "test_df = preprocessing_pipeline.transform(scooby_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the column names of the dataset for the Regressor, we are going to exclude the None values\n",
    "\n",
    "CAT_COLS = [k for k in train_df.columns if ('_NORM' in k) & ('None' not in k)]\n",
    "NUM_COLS = [k for k in train_df.columns if ('_OHE' in k) & ('None' not in k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBRegressor\n",
    "regressor = XGBRegressor(\n",
    "    input_cols=CAT_COLS + NUM_COLS,\n",
    "    label_cols=LABEL_COLUMNS,\n",
    "    output_cols=OUTPUT_COLUMNS\n",
    ")\n",
    "\n",
    "# Train\n",
    "regressor.fit(train_df)\n",
    "\n",
    "# Predict\n",
    "result = regressor.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can analyse the results using Snowpark ML's MAPE\n",
    "\n",
    "mape = mean_absolute_percentage_error(df=result, \n",
    "                                        y_true_col_names=\"IMDB\", \n",
    "                                        y_pred_col_names=\"IMDB_PRICE\")\n",
    "\n",
    "result.select(\"IMDB\", \"IMDB_PRICE\").show()\n",
    "print(f\"Mean absolute percentage error: {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted \n",
    "g = sns.relplot(data=result[\"IMDB\", \"IMDB_PRICE\"].to_pandas().astype(\"float64\"), x=\"IMDB\", y=\"IMDB_PRICE\", kind=\"scatter\")\n",
    "g.ax.axline((0,0), slope=1, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's use Snowpark ML's `GridSearchCV()` function to find optimal model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBRegressor(),\n",
    "    param_grid={\n",
    "        \"n_estimators\":[100, 200, 300, 400, 500],\n",
    "        \"learning_rate\":[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    },\n",
    "    n_jobs = -1,\n",
    "    scoring=\"neg_mean_absolute_percentage_error\",\n",
    "    input_cols= CAT_COLS + NUM_COLS,\n",
    "    label_cols=LABEL_COLUMNS,\n",
    "    output_cols=OUTPUT_COLUMNS\n",
    ")\n",
    "\n",
    "# Train\n",
    "grid_search.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the best estimator has the following parameters: `n_estimators=100` & `learning_rate=0.1`.\n",
    "\n",
    "We can use `to_sklearn()` in order to get the actual xgboost model object, which gives us access to all its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.to_sklearn().best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze grid search results\n",
    "gs_results = grid_search.to_sklearn().cv_results_\n",
    "n_estimators_val = []\n",
    "learning_rate_val = []\n",
    "for param_dict in gs_results[\"params\"]:\n",
    "    n_estimators_val.append(param_dict[\"n_estimators\"])\n",
    "    learning_rate_val.append(param_dict[\"learning_rate\"])\n",
    "mape_val = gs_results[\"mean_test_score\"]*-1\n",
    "\n",
    "gs_results_df = pd.DataFrame(data={\n",
    "    \"n_estimators\":n_estimators_val,\n",
    "    \"learning_rate\":learning_rate_val,\n",
    "    \"mape\":mape_val})\n",
    "\n",
    "sns.relplot(data=gs_results_df, x=\"learning_rate\", y=\"mape\", hue=\"n_estimators\", kind=\"line\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is consistent with the `learning_rate=0.1` and `n_estimator=100` chosen as the best estimator with the lowest MAPE.\n",
    "<br>\n",
    "Next we predict and analyse the results using the best estimator <br>\n",
    "\n",
    "The previous mape was: 0.05170313160722504 <br>\n",
    "With the best estimator it is: 0.050110345648975284\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "result = grid_search.predict(test_df)\n",
    "\n",
    "# Analyze results\n",
    "mape = mean_absolute_percentage_error(df=result, \n",
    "                                        y_true_col_names=\"IMDB\", \n",
    "                                        y_pred_col_names=\"IMDB_PRICE\")\n",
    "\n",
    "result.select(\"IMDB\", \"IMDB_PRICE\").show()\n",
    "print(f\"Mean absolute percentage error: {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted \n",
    "g = sns.relplot(data=result[\"IMDB\", \"IMDB_PRICE\"].to_pandas().astype(\"float64\"), x=\"IMDB\", y=\"IMDB_PRICE\", kind=\"scatter\")\n",
    "g.ax.axline((0,0), slope=1, color=\"r\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model deployment using permanent Vectorised User-Defined Function (UDF)\n",
    "\n",
    "One thing to keep in mind is that Snowpark ML's `model.predict()` function creates a ***temporary*** UDF, so in order to persist as a permanent UDF, we'll need to define our own UDF. \n",
    "\n",
    "***Note: Once Snowpark ML's native model registry is available, this will be the more streamlined approach to deploy your model.***\n",
    "\n",
    "We will save the model as an sklearn object so it can be used externally. This is how we'll deploy the model as a UDF in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save our optimal model first\n",
    "optimal_model = grid_search.to_sklearn()\n",
    "MODEL_FILE = 'model.joblib'\n",
    "joblib.dump(optimal_model, MODEL_FILE) # we are just pickling it locally first\n",
    "\n",
    "# You can also save the pickled object into the stage we created earlier\n",
    "session.file.put(MODEL_FILE, \"@SCOOBY_ASSETS\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all relevant column names to pass into the UDF call\n",
    "feature_cols = test_df[CAT_COLS + NUM_COLS].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vectorised User Defined Function (UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the model load to optimize inference\n",
    "@cachetools.cached(cache={})\n",
    "def load_model(filename):\n",
    "    import joblib\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    IMPORT_DIRECTORY_NAME = \"snowflake_import_directory\"\n",
    "    import_dir = sys._xoptions[IMPORT_DIRECTORY_NAME]\n",
    "\n",
    "    if import_dir:\n",
    "        with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "            m = joblib.load(file)\n",
    "            return m\n",
    "\n",
    "# Register the UDF via decorator\n",
    "@udf(name='batch_predict_imdb', \n",
    "     session=session, \n",
    "     replace=True, \n",
    "     is_permanent=True, \n",
    "     stage_location='@SCOOBY_ASSETS',\n",
    "     input_types=[F.FloatType()]*len(feature_cols),\n",
    "     return_type=F.FloatType(),\n",
    "     imports=['@SCOOBY_ASSETS/model.joblib.gz'],\n",
    "     packages=['pandas','joblib','cachetools','xgboost'])\n",
    "def batch_predict_imdb(test_df: pd.DataFrame) -> pd.Series:\n",
    "    # Need to name the columns because column names aren't passed in to this function\n",
    "    test_df.columns = CAT_COLS + NUM_COLS\n",
    "    model = load_model('model.joblib.gz')\n",
    "    return model.predict(test_df) # This is using the XGBoost library's model.predict(), not Snowpark ML's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call Vectorized User-Defined Function (UDF) on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_w_preds = test_df.with_column('PREDICTED_IMDB', batch_predict_imdb(*feature_cols))\n",
    "test_df_w_preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions to a Snowflake table\n",
    "test_df_w_preds.write.mode('overwrite').save_as_table('scooby_w_predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark-ml-scooby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
