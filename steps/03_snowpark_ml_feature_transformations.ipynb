{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ML Feature Transformations\n",
    "\n",
    "In this notebook, we will walk through a few transformations that are included in the Snowpark ML Preprocessing API. <br>\n",
    "We will also build a preprocessing pipeline to be used in the ML modeling notebook. Having a preprocessing pipeline is very useful to be able to apply in a standard way the same treatment to train, test and data to be scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark for Python\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.snowpark.version import VERSION\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.types import DecimalType\n",
    "\n",
    "# Snowpark ML\n",
    "import snowflake.ml.modeling.preprocessing as snowml\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.metrics.correlation import correlation\n",
    "\n",
    "# Data Science Libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Misc\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# warning suppresion\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Establish Secure Connection to Snowflake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('creds.json') as f:\n",
    "    connection_parameters = json.load(f)\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "\n",
    "snowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('\\nConnection Established with the following parameters:')\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the table name where we stored the scooby_doo dataset\n",
    "DEMO_TABLE = 'scooby_clean'\n",
    "input_tbl = f\"{session.get_current_database()}.{session.get_current_schema()}.{DEMO_TABLE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load the data from snowflake and select the features to be used in our ML model, few things I considered for this selection:\n",
    "<ol>\n",
    "\n",
    "<li>For categorical features, I dismissed the ones that have very sparse values. Given that we have 604 observations, I dismissed the features with more than 30 categories, in this way we'll have at least 20 possible observations per category in average. <br>\n",
    "The selected features are: <code>`FORMAT\",\"NETWORK\",\"SETTING_TERRAIN\",\"MOTIVE\",\"MONSTER_GENDER\",\"CULPRIT_GENDER\" </code> </li>\n",
    "\n",
    "<br>\n",
    "<li>For numerical features I keep <code>\"IMDB\"</code> as it's our target feature. <br>\n",
    "I keep <code>\"ENGAGEMENT\"</code> and <code>\"RUN_TIME\" </code>which is the number of minutes of the episode. <br> <br>\n",
    "\n",
    "For the rest of the integers I decide that as a minumum, 80% of the observations should contain non-null values\n",
    "I keep <code>\"MONSTER_AMOUNT\",\"SUSPECTS_AMOUNT\",\"CULPRIT_AMOUNT\" </code><br>\n",
    "And I keep the features that indicate how many times each of these phrases were said during the episode: <br> \n",
    "<code> \"SPLIT_UP\",\"ANOTHER_MYSTERY\",\"SET_A_TRAP\",\"JEEPERS\",\"JINKIES\",\"MY_GLASSES\" <br>\n",
    ",\"JUST_ABOUT_WRAPPED_UP\",\"ZOINKS\",\"GROOVY\",\"SCOOBY_DOO_WHERE_ARE_YOU\",\"ROOBY_ROOBY_ROO\" </code> </li>\n",
    "\n",
    "<br>\n",
    "<li>For boolean features I will keep as well only the ones that have non-null values in at least 80% of the rows <br>\n",
    "Additionally the features caught, captured and unmasked give pretty similar information, so we'll keep only caught features <br>\n",
    "I keep <code>\"MONSTER_REAL\",\"CAUGHT_SHAGGY\",\"CAUGHT_SCOOBY\",\"SNACK_SHAGGY\",\"SNACK_SCOOBY\",\"UNMASK_OTHER\",\"CAUGHT_OTHER\", <br>\n",
    "\"CAUGHT_NOT\",\"DOOR_GAG\",\"BATMAN\",\"SCOOBY_DUM\",\"SCRAPPY_DOO\",\"HEX_GIRLS\",\"BLUE_FALCON\"</code> </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we read in the data from a Snowflake table into a Snowpark DataFrame\n",
    "scooby_df = session.table(input_tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Filter out rows where IMDB is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scooby_df = scooby_df.filter(F.not_(F.is_null(F.col(\"IMDB\"))))\n",
    "print(scooby_df.count())\n",
    "scooby_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Select the features we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the features that we are going to use in the ML model according to the analysis below\n",
    "features_array = [\"IMDB\",\"ENGAGEMENT\",\"RUN_TIME\",\"FORMAT\",\"NETWORK\",\"SETTING_TERRAIN\",\"MOTIVE\",\"MONSTER_GENDER\",\"CULPRIT_GENDER\"\n",
    ",\"MONSTER_AMOUNT\",\"SUSPECTS_AMOUNT\",\"CULPRIT_AMOUNT\",\"ZOINKS\",\"GROOVY\",\"SCOOBY_DOO_WHERE_ARE_YOU\",\"ROOBY_ROOBY_ROO\"\n",
    ",\"MONSTER_REAL\",\"CAUGHT_SHAGGY\",\"CAUGHT_SCOOBY\",\"SNACK_SHAGGY\",\"SNACK_SCOOBY\",\"UNMASK_OTHER\",\"CAUGHT_OTHER\",\"CAUGHT_NOT\"\n",
    ",\"DOOR_GAG\",\"BATMAN\",\"SCOOBY_DUM\",\"SCRAPPY_DOO\",\"HEX_GIRLS\",\"BLUE_FALCON\"]\n",
    "\n",
    "scooby_ml_df = scooby_df.select(features_array)\n",
    "scooby_ml_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis to identify candidate features to use\n",
    "\n",
    "This analysis is informative, NO need to run it once the features have been identified\n",
    "\n",
    "### Analyse category features to identify the ones that have < 30 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Columns review\n",
    "cat_array = [\"FORMAT\",\"NETWORK\",\"SETTING_TERRAIN\",\"SETTING_COUNTRY_STATE\",\"MOTIVE\"\n",
    "             ,\"MONSTER_GENDER\",\"MONSTER_TYPE\",\"MONSTER_SUBTYPE\",\"MONSTER_SPECIES\",\"CULPRIT_GENDER\"]\n",
    "\n",
    "for c in cat_array:\n",
    "    print(c + \" \" + str(len(set(scooby_df.select(F.col(c)).collect()))))\n",
    "\n",
    "print(scooby_df.count())\n",
    "print(set(scooby_df.select(F.col(\"FORMAT\")).collect()))\n",
    "print(set(scooby_df.select(F.col(\"NETWORK\")).collect()))\n",
    "print(set(scooby_df.select(F.col(\"SETTING_TERRAIN\")).collect()))\n",
    "print(set(scooby_df.select(F.col(\"SETTING_COUNTRY_STATE\")).collect()))\n",
    "print(set(scooby_df.select(F.col(\"MOTIVE\")).collect()))\n",
    "\n",
    "\n",
    "print(set(scooby_df.select(F.col(\"MONSTER_GENDER\")).collect()))\n",
    "print(set(scooby_df.select(F.col(\"MONSTER_TYPE\")).collect()))\n",
    "print(set(scooby_df.select(F.col(\"MONSTER_SUBTYPE\")).collect()))\n",
    "print(set(scooby_df.select(F.col(\"MONSTER_SPECIES\")).collect()))\n",
    "print(set(scooby_df.select(F.col(\"CULPRIT_GENDER\")).collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical and boolean features\n",
    "\n",
    "For numerical and boolean features we want at least 80% of the dataset containing a value, given that we have 604 observations we need at least 483 non-null values per feature to consider it. <br>\n",
    "We can investigate the dataset with a describe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays from the 02_snowpark_ml_data_ingest.ipynb cast types step\n",
    "int_array = [\"ENGAGEMENT\",\"RUN_TIME\",\"MONSTER_AMOUNT\",\"SUSPECTS_AMOUNT\",\"CULPRIT_AMOUNT\",\"SPLIT_UP\",\"ANOTHER_MYSTERY\",\"SET_A_TRAP\",\"JEEPERS\",\"JINKIES\",\"MY_GLASSES\"\n",
    ",\"JUST_ABOUT_WRAPPED_UP\",\"ZOINKS\",\"GROOVY\",\"SCOOBY_DOO_WHERE_ARE_YOU\",\"ROOBY_ROOBY_ROO\"]\n",
    "float_array = [\"IMDB\"]\n",
    "boolean_array = [\"MONSTER_REAL\",\"CAUGHT_FRED\",\"CAUGHT_DAPHNE\",\"CAUGHT_VELMA\",\"CAUGHT_SHAGGY\",\"CAUGHT_SCOOBY\"\n",
    ",\"CAPTURED_FRED\",\"CAPTURED_DAPHNE\",\"CAPTURED_VELMA\",\"CAPTURED_SHAGGY\",\"CAPTURED_SCOOBY\"\n",
    ",\"UNMASK_FRED\",\"UNMASK_DAPHNE\",\"UNMASK_VELMA\",\"UNMASK_SHAGGY\",\"UNMASK_SCOOBY\"\n",
    ",\"SNACK_FRED\",\"SNACK_DAPHNE\",\"SNACK_VELMA\",\"SNACK_SHAGGY\",\"SNACK_SCOOBY\",\"UNMASK_OTHER\",\"CAUGHT_OTHER\",\"CAUGHT_NOT\",\"TRAP_WORK_FIRST\",\"NON_SUSPECT\",\"ARRESTED\",\"DOOR_GAG\"\n",
    ",\"BATMAN\",\"SCOOBY_DUM\",\"SCRAPPY_DOO\",\"HEX_GIRLS\",\"BLUE_FALCON\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scooby_df.select(int_array).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark dataframe describe doesn't work with boolean data types at the moment, so we create a function to investigate this. We'll dismiss those that have > 120 NULL (20% of 604)\n",
    "# Open feature request for support of non-numeric and non-string data types: https://github.com/snowflakedb/snowpark-python/issues/1016\n",
    "\n",
    "# scooby_df.select(boolean_array).describe(include=\"all\").show()\n",
    "def desc_bools(col):\n",
    "    print(col)\n",
    "    scooby_df.group_by(col).count().show()\n",
    "\n",
    "for c in boolean_array:\n",
    "    desc_bools(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Transformations\n",
    "\n",
    "We will illustrate a few of the transformation functions here, but the rest can be found in the [documentation](https://docs.snowflake.com/LIMITEDACCESS/snowflake-ml-preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scooby_ml_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scooby_ml_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Categorical values to numerical features\n",
    "We use the `OneHotEncoder` to transform the categorical values: `FORMAT`, `NETWORK`,`SETTING_TERRAIN`, `MOTIVE`, `MONSTER_GENDER`,`CULPRIT_GENDER`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categoricals to numeric columns\n",
    "cat_array2 = [\"FORMAT\",\"NETWORK\",\"SETTING_TERRAIN\",\"MOTIVE\",\"MONSTER_GENDER\",\"CULPRIT_GENDER\"]\n",
    "cat_array_ohe = [\"FORMAT_OHE\",\"NETWORK_OHE\",\"SETTING_TERRAIN_OHE\",\"MOTIVE_OHE\",\"MONSTER_GENDER_OHE\",\"CULPRIT_GENDER_OHE\"]\n",
    "\n",
    "snowml_ohe = snowml.OneHotEncoder(input_cols=cat_array2, output_cols=cat_array_ohe)\n",
    "ohe_scooby_df = snowml_ohe.fit(scooby_ml_df).transform(scooby_ml_df)\n",
    "\n",
    "np.array(ohe_scooby_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_scooby_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalize numerical features\n",
    "\n",
    "Use `MinMaxScaler` to normalize the numerical features with large differences between their min and max values: `ENGAGEMENT`, `RUN_TIME`, `ZOINKS`, `GROOVY`, `SCOOBY_DOO_WHERE_ARE_YOU`, \n",
    "`ROOBY_ROOBY_ROO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_array_2 = [\"ENGAGEMENT\",\"RUN_TIME\",\"ZOINKS\",\"GROOVY\",\"SCOOBY_DOO_WHERE_ARE_YOU\",\"ROOBY_ROOBY_ROO\"]\n",
    "int_array_norm = [\"ENGAGEMENT_NORM\",\"RUN_TIME_NORM\",\"ZOINKS_NORM\",\"GROOVY_NORM\",\"SCOOBY_DOO_WHERE_ARE_YOU_NORM\",\"ROOBY_ROOBY_ROO_NORM\"]\n",
    "\n",
    "# Normalize the CARAT column\n",
    "snowml_mms = snowml.MinMaxScaler(input_cols=int_array_2, output_cols=int_array_norm)\n",
    "normalized_scooby_df = snowml_mms.fit(ohe_scooby_df).transform(ohe_scooby_df)\n",
    "\n",
    "# Reduce the number of decimals\n",
    "for c in int_array_norm:\n",
    "    new_col = normalized_scooby_df.col(c).cast(DecimalType(7, 6))\n",
    "    normalized_scooby_df = normalized_scooby_df.with_column(c, new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_scooby_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build the full preprocessing Pipeline\n",
    "\n",
    "Having a preprocessing pipeline is helpful both for training and inference to have standardised steps for feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"FORMAT\",\"NETWORK\",\"SETTING_TERRAIN\",\"MOTIVE\",\"MONSTER_GENDER\",\"CULPRIT_GENDER\"]\n",
    "CATEGORICAL_COLUMNS_OE = [\"FORMAT_OHE\",\"NETWORK_OHE\",\"SETTING_TERRAIN_OHE\",\"MOTIVE_OHE\",\"MONSTER_GENDER_OHE\",\"CULPRIT_GENDER_OHE\"] # To name the ordinal encoded columns\n",
    "\n",
    "NUMERICAL_COLUMNS = [\"ENGAGEMENT\",\"RUN_TIME\",\"ZOINKS\",\"GROOVY\",\"SCOOBY_DOO_WHERE_ARE_YOU\",\"ROOBY_ROOBY_ROO\"]\n",
    "NUMERICAL_COLUMNS_NORM = [\"ENGAGEMENT_NORM\",\"RUN_TIME_NORM\",\"ZOINKS_NORM\",\"GROOVY_NORM\",\"SCOOBY_DOO_WHERE_ARE_YOU_NORM\",\"ROOBY_ROOBY_ROO_NORM\"]\n",
    "\n",
    "# Build the pipeline\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    steps=[\n",
    "            (\n",
    "                \"OHE\",\n",
    "                snowml.OneHotEncoder(\n",
    "                    input_cols=CATEGORICAL_COLUMNS,\n",
    "                    output_cols=CATEGORICAL_COLUMNS_OE\n",
    "                )\n",
    "            ),\n",
    "            (\n",
    "                \"MMS\",\n",
    "                snowml.MinMaxScaler(\n",
    "                    clip=True,\n",
    "                    input_cols=NUMERICAL_COLUMNS,\n",
    "                    output_cols=NUMERICAL_COLUMNS_NORM,\n",
    "                )\n",
    "            )\n",
    "    ]\n",
    ")\n",
    "\n",
    "PIPELINE_FILE = 'preprocessing_pipeline.joblib'\n",
    "joblib.dump(preprocessing_pipeline, PIPELINE_FILE) # We are just pickling it locally first\n",
    "\n",
    "transformed_scooby_df = preprocessing_pipeline.fit(scooby_ml_df).transform(scooby_ml_df)\n",
    "transformed_scooby_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also save the pickled object into the stage we created earlier for deployment\n",
    "session.file.put(PIPELINE_FILE, \"@SCOOBY_ASSETS\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_scooby_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_features = [\"IMDB\",\"ENGAGEMENT_NORM\",\"RUN_TIME_NORM\",\n",
    "\"FORMAT_OHE_CROSSOVER\",\"FORMAT_OHE_MOVIE\",\"FORMAT_OHE_MOVIE_THEATRICAL_\",\"FORMAT_OHE_TV_SERIES\",\"FORMAT_OHE_TV_SERIES_SEGMENTED_\",\n",
    "\"NETWORK_OHE_ABC\",\"NETWORK_OHE_ADULT_SWIM\",\"NETWORK_OHE_BOOMERANG\",\"NETWORK_OHE_CARTOON_NETWORK\",\"NETWORK_OHE_CBS\",\"NETWORK_OHE_SYNDICATION\",\n",
    "\"NETWORK_OHE_TBC\",\"NETWORK_OHE_THE_CW\",\"NETWORK_OHE_THE_WB\",\"NETWORK_OHE_WARNER_BROS_PICTURE\",\"NETWORK_OHE_WARNER_HOME_VIDEO\"]\n",
    "\n",
    "simplified_scooby_df = transformed_scooby_df[ml_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_scooby_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_scooby_df = correlation(df=simplified_scooby_df)\n",
    "corr_scooby_df # This is a Pandas DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap with the features\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plt.title('Heatmap for Transformed Scooby Data', fontsize=28)\n",
    "dataplot = sns.heatmap(corr_scooby_df, cmap=\"YlGnBu\", annot=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not very high correlation with IMDB in any of the features, but we can see that the highest correlation is for the Format = TV_Series of 0.41. <br>\n",
    "We used a one hot encoding technique to transform the `FORMAT` categorical value, into a series of continuous variables; in this way we can apply correlation checks to it. <br>\n",
    "<br>\n",
    "In the following scatterplot, given that the TV_Series feature is binary (1 or 0) we will see that the graphic is not very useful to portrait any kind of correlation. <br>\n",
    "Another way to calculate a correlation between a binary variable and a continuous one is to use a point biserial correlation. <br>\n",
    "https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.pointbiserialr.html <br>\n",
    "https://www.statology.org/point-biserial-correlation-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a plot to look at FORMAT_OHE_TV_SERIES and IMDB\n",
    "counts = simplified_scooby_df.to_pandas().groupby(['IMDB', 'FORMAT_OHE_TV_SERIES']).size().reset_index(name='Count')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax = sns.scatterplot(data=counts, x='FORMAT_OHE_TV_SERIES', y='IMDB', size='Count', markers='o', alpha=(0.1, .25, 0.5, 0.75, 1))\n",
    "ax.grid(axis='y')\n",
    "\n",
    "sns.move_legend(ax, \"upper right\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the scipy.stats library, we can calculate the point biserial correlation for `FORMAT_OHE_TV_SERIES`. <br>\n",
    "The correlation is 0.41 and the p-value is 0.05 which means that it is statistically significant. <br> <br>\n",
    "\n",
    "In the same way, for testing purposes we calculate for `NETWORK_OHE_THE_CW`. <br>\n",
    "This has a negative correlation of -0.45 <br>\n",
    "\n",
    "When the correlation is positive, this indicates that when the variable x takes on the value “1” that the variable y tends to take on higher values compared to when the variable x takes on the value “0”. We can then interpret this as the episodes with FORMAT = TV_SERIES having in general a higher IMDB than the episodes with FORMAT = THE_CW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = simplified_scooby_df.select(F.col(\"FORMAT_OHE_TV_SERIES\")).toPandas().to_numpy().flatten()\n",
    "y = simplified_scooby_df.select(F.col(\"IMDB\")).toPandas().to_numpy().flatten()\n",
    "print(\"Correlation for FORMAT_OHE_TV_SERIES\")\n",
    "print(stats.pointbiserialr(x, y))\n",
    "\n",
    "x = simplified_scooby_df.select(F.col(\"NETWORK_OHE_THE_CW\")).toPandas().to_numpy().flatten()\n",
    "y = simplified_scooby_df.select(F.col(\"IMDB\")).toPandas().to_numpy().flatten()\n",
    "print(\"Correlation for NETWORK_OHE_THE_CW\")\n",
    "print(stats.pointbiserialr(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target variable `IMDB` is a continuous variable. <br>\n",
    "The other 2 natural continuous variables are: `ENGAGEMENT_NORM` and `RUN_TIME_NORM`. <br>\n",
    "The correlation heatmap indicates a negative very low correlation between IMDB and these 2, let's explore that in a visual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a plot to look at ENGAGEMENT_NORM and IMDB\n",
    "counts = simplified_scooby_df.to_pandas().groupby(['IMDB', 'ENGAGEMENT_NORM']).size().reset_index(name='Count')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax = sns.scatterplot(data=counts, x='ENGAGEMENT_NORM', y='IMDB', size='Count', markers='o', alpha=(0.1, .25, 0.5, 0.75, 1))\n",
    "ax.grid(axis='y')\n",
    "\n",
    "\n",
    "sns.move_legend(ax, \"upper right\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a plot to look at RUN_TIME_NORM and IMDB\n",
    "counts = simplified_scooby_df.to_pandas().groupby(['IMDB', 'RUN_TIME_NORM']).size().reset_index(name='Count')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax = sns.scatterplot(data=counts, x='RUN_TIME_NORM', y='IMDB', size='Count', markers='o', alpha=(0.1, .25, 0.5, 0.75, 1))\n",
    "ax.grid(axis='y')\n",
    "\n",
    "\n",
    "sns.move_legend(ax, \"upper right\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a plot to look at RUN_TIME_NORM and ENGAGEMENT_NORM\n",
    "counts = simplified_scooby_df.to_pandas().groupby(['ENGAGEMENT_NORM', 'RUN_TIME_NORM']).size().reset_index(name='Count')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax = sns.scatterplot(data=counts, x='RUN_TIME_NORM', y='ENGAGEMENT_NORM', size='Count', markers='o', alpha=(0.1, .25, 0.5, 0.75, 1))\n",
    "ax.grid(axis='y')\n",
    "\n",
    "sns.move_legend(ax, \"upper right\")\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowpark-ml-scooby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
